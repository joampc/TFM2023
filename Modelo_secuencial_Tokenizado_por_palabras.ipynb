{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-17T19:48:39.976375Z","iopub.execute_input":"2023-09-17T19:48:39.976819Z","iopub.status.idle":"2023-09-17T19:48:39.987448Z","shell.execute_reply.started":"2023-09-17T19:48:39.976789Z","shell.execute_reply":"2023-09-17T19:48:39.986451Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport multiprocessing\nfrom functools import partial\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, RepeatVector\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nimport random\n#from googletrans import Translator\nfrom scipy.special import softmax","metadata":{"execution":{"iopub.status.busy":"2023-09-17T19:48:49.766569Z","iopub.execute_input":"2023-09-17T19:48:49.766940Z","iopub.status.idle":"2023-09-17T19:49:06.768728Z","shell.execute_reply.started":"2023-09-17T19:48:49.766909Z","shell.execute_reply":"2023-09-17T19:49:06.767673Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"df_raw = pd.read_csv('/kaggle/input/dataset-recipes/RAW_recipes.csv')\n\nprint(f'Número de filas: {df_raw.shape[0]}, Número de columnas:{df_raw.shape[1]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T19:50:04.613766Z","iopub.execute_input":"2023-09-17T19:50:04.614686Z","iopub.status.idle":"2023-09-17T19:50:12.491951Z","shell.execute_reply.started":"2023-09-17T19:50:04.614640Z","shell.execute_reply":"2023-09-17T19:50:12.490880Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Número de filas: 231637, Número de columnas:12\n","output_type":"stream"}]},{"cell_type":"code","source":"df_raw.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-17T19:50:15.996468Z","iopub.execute_input":"2023-09-17T19:50:15.996839Z","iopub.status.idle":"2023-09-17T19:50:16.004846Z","shell.execute_reply.started":"2023-09-17T19:50:15.996808Z","shell.execute_reply":"2023-09-17T19:50:16.003807Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags',\n       'nutrition', 'n_steps', 'steps', 'description', 'ingredients',\n       'n_ingredients'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"def check_ingredients_in_steps(ingredients, steps):\n    for ingredient in ingredients:\n        if ingredient not in ' '.join(steps):\n            return False\n    return True\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T19:59:42.792276Z","iopub.execute_input":"2023-09-17T19:59:42.792666Z","iopub.status.idle":"2023-09-17T19:59:42.797590Z","shell.execute_reply.started":"2023-09-17T19:59:42.792634Z","shell.execute_reply":"2023-09-17T19:59:42.796479Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_raw['all_ingredients_in_steps'] = df_raw.apply(lambda row: check_ingredients_in_steps(row['ingredients'], row['steps']), axis=1)\ncount = df_raw['all_ingredients_in_steps'].sum()\nprint(f'Número de registros en los que todos los ingredientes están incluidos en los pasos: {count}')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T19:59:44.628666Z","iopub.execute_input":"2023-09-17T19:59:44.629032Z","iopub.status.idle":"2023-09-17T20:12:26.300086Z","shell.execute_reply.started":"2023-09-17T19:59:44.629002Z","shell.execute_reply":"2023-09-17T20:12:26.298978Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Número de registros en los que todos los ingredientes están incluidos en los pasos: 150793\n","output_type":"stream"}]},{"cell_type":"code","source":"filtered_df = df_raw[df_raw['all_ingredients_in_steps']]","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:18:30.841465Z","iopub.execute_input":"2023-09-17T20:18:30.841851Z","iopub.status.idle":"2023-09-17T20:18:30.888448Z","shell.execute_reply.started":"2023-09-17T20:18:30.841820Z","shell.execute_reply":"2023-09-17T20:18:30.887450Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df = filtered_df[:10000]\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:18:35.847506Z","iopub.execute_input":"2023-09-17T20:18:35.847886Z","iopub.status.idle":"2023-09-17T20:18:35.855297Z","shell.execute_reply.started":"2023-09-17T20:18:35.847857Z","shell.execute_reply":"2023-09-17T20:18:35.854258Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"10000"},"metadata":{}}]},{"cell_type":"markdown","source":"**Tokenizacion**","metadata":{}},{"cell_type":"code","source":"#Definimos una función para tokenizar una lista de texto (ingredientes o pasos)\ndef tokenize_list(text_list):\n    # Removemos los corchetes y comillas del texto y luego dividimos por la coma\n    return [ingredient.lower().strip().strip(\"'\") for ingredient in text_list.strip(\"[]\").split(\",\")]\n\n\ndf['ingredient_tokens'] = df['ingredients'].apply(tokenize_list)\ndf['steps_tokens'] = df['steps'].apply(tokenize_list)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:18:44.826749Z","iopub.execute_input":"2023-09-17T20:18:44.827212Z","iopub.status.idle":"2023-09-17T20:18:44.998949Z","shell.execute_reply.started":"2023-09-17T20:18:44.827169Z","shell.execute_reply":"2023-09-17T20:18:44.997960Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/2618552342.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['ingredient_tokens'] = df['ingredients'].apply(tokenize_list)\n/tmp/ipykernel_28/2618552342.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['steps_tokens'] = df['steps'].apply(tokenize_list)\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\nprint(df.loc[:1, 'ingredients'])\nprint(df.loc[:1, 'ingredient_tokens'])\n\nprint(df.loc[:1, 'steps'])\nprint(df.loc[:1, 'steps_tokens'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:05:58.596362Z","iopub.execute_input":"2023-09-15T23:05:58.596807Z","iopub.status.idle":"2023-09-15T23:05:58.614874Z","shell.execute_reply.started":"2023-09-15T23:05:58.596765Z","shell.execute_reply":"2023-09-15T23:05:58.613871Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0    ['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'butter', 'olive oil', 'salt']\n1           ['prepared pizza crust', 'sausage patty', 'eggs', 'milk', 'salt and pepper', 'cheese']\nName: ingredients, dtype: object\n0    [winter squash, mexican seasoning, mixed spice, honey, butter, olive oil, salt]\n1         [prepared pizza crust, sausage patty, eggs, milk, salt and pepper, cheese]\nName: ingredient_tokens, dtype: object\n0    ['make a choice and proceed with recipe', 'depending on size of squash , cut into half or fourths', 'remove seeds', 'for spicy squash , drizzle olive oil or melted butter over each cut squash piece', 'season with mexican seasoning mix ii', 'for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece', 'season with sweet mexican spice mix', 'bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin', 'be careful not to burn the squash especially if you opt to use sugar or butter', 'if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking', 'if desired , season with salt']\n1                                                                                                                                                                                                                                                                                                                                 ['preheat oven to 425 degrees f', 'press dough into the bottom and sides of a 12 inch pizza pan', 'bake for 5 minutes until set but not browned', 'cut sausage into small pieces', 'whisk eggs and milk in a bowl until frothy', 'spoon sausage over baked crust and sprinkle with cheese', 'pour egg mixture slowly over sausage and cheese', 's& p to taste', 'bake 15-20 minutes or until eggs are set and crust is brown']\nName: steps, dtype: object\n0    [make a choice and proceed with recipe, depending on size of squash, cut into half or fourths, remove seeds, for spicy squash, drizzle olive oil or melted butter over each cut squash piece, season with mexican seasoning mix ii, for sweet squash, drizzle melted honey, butter, grated piloncillo over each cut squash piece, season with sweet mexican spice mix, bake at 350 degrees, again depending on size, for 40 minutes up to an hour, until a fork can easily pierce the skin, be careful not to burn the squash especially if you opt to use sugar or butter, if you feel more comfortable, cover the squash with aluminum foil the first half hour, give or take, of baking, if desired, season with salt]\n1                                                                                                                                                                                                                                                                                                                 [preheat oven to 425 degrees f, press dough into the bottom and sides of a 12 inch pizza pan, bake for 5 minutes until set but not browned, cut sausage into small pieces, whisk eggs and milk in a bowl until frothy, spoon sausage over baked crust and sprinkle with cheese, pour egg mixture slowly over sausage and cheese, s& p to taste, bake 15-20 minutes or until eggs are set and crust is brown]\nName: steps_tokens, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Creacion de Vocabulario**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Creación del vocabulario\nall_texts = df['ingredient_tokens'].tolist() + df['steps_tokens'].tolist()\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_texts)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:18:56.883296Z","iopub.execute_input":"2023-09-17T20:18:56.883669Z","iopub.status.idle":"2023-09-17T20:18:57.329121Z","shell.execute_reply.started":"2023-09-17T20:18:56.883638Z","shell.execute_reply":"2023-09-17T20:18:57.328119Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Creacion de secuencias numericas","metadata":{}},{"cell_type":"code","source":"# Creación de secuencias numéricas\ningredient_sequences = tokenizer.texts_to_sequences(df['ingredient_tokens'])\nsteps_sequences = tokenizer.texts_to_sequences(df['steps_tokens'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:19:00.357075Z","iopub.execute_input":"2023-09-17T20:19:00.357472Z","iopub.status.idle":"2023-09-17T20:19:00.849783Z","shell.execute_reply.started":"2023-09-17T20:19:00.357439Z","shell.execute_reply":"2023-09-17T20:19:00.848755Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Padding","metadata":{}},{"cell_type":"code","source":"# Padding\nmax_sequence_length = max(max(len(seq) for seq in ingredient_sequences), max(len(seq) for seq in steps_sequences))\ningredient_sequences = pad_sequences(ingredient_sequences, maxlen=max_sequence_length, padding='post')\nsteps_sequences = pad_sequences(steps_sequences, maxlen=max_sequence_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:19:03.511632Z","iopub.execute_input":"2023-09-17T20:19:03.511984Z","iopub.status.idle":"2023-09-17T20:19:03.626192Z","shell.execute_reply.started":"2023-09-17T20:19:03.511954Z","shell.execute_reply":"2023-09-17T20:19:03.625255Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### División de datos","metadata":{}},{"cell_type":"code","source":"# División de datos en conjuntos de entrenamiento, validación y prueba\n\ntrain_size = int(0.8 * len(df_raw))\nval_size = int(0.1 * len(df_raw))\ntrain_ingredients = ingredient_sequences[:train_size]\ntrain_steps = steps_sequences[:train_size]\nval_ingredients = ingredient_sequences[train_size:train_size+val_size]\nval_steps = steps_sequences[train_size:train_size+val_size]\ntest_ingredients = ingredient_sequences[train_size+val_size:]\ntest_steps = steps_sequences[train_size+val_size:]\n\n\n# También puedes obtener el vocabulario creado por el Tokenizer y los índices de palabras utilizando:\nword_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:19:30.571585Z","iopub.execute_input":"2023-09-17T20:19:30.572219Z","iopub.status.idle":"2023-09-17T20:19:30.582654Z","shell.execute_reply.started":"2023-09-17T20:19:30.572176Z","shell.execute_reply":"2023-09-17T20:19:30.581480Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Contrucción del Modelo y Training**","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, LSTM, Dense\nimport numpy as np\n\n# Creación del vocabulario\nall_texts = df['ingredient_tokens'].tolist() + df['steps_tokens'].tolist()\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_texts)\n\n# Verificar si '<start>' y '<end>' están presentes en el vocabulario, si no, agrégales manualmente\nif '<start>' not in tokenizer.word_index:\n    tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n    tokenizer.index_word[len(tokenizer.word_index)] = '<start>'\n\nif '<end>' not in tokenizer.word_index:\n    tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1\n    tokenizer.index_word[len(tokenizer.word_index)] = '<end>'\n\n# Obtener el tamaño del vocabulario del tokenizer\nvocab_size = len(tokenizer.word_index) + 1\n\n# Asegurar que la longitud máxima de ingredientes y pasos sea la misma\nmax_sequence_length = 200\n\n# Rellenar secuencias con ceros para que todas tengan la misma longitud\ntrain_ingredients_padded = pad_sequences(train_ingredients, maxlen=max_sequence_length, padding='post')\ntrain_steps_padded = pad_sequences(train_steps, maxlen=max_sequence_length, padding='post')\n\n# Definir la arquitectura del modelo codificador-decodificador\nembedding_dim = 100\nrnn_units = 128\n\nencoder_input = Input(shape=(max_sequence_length,))\nencoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_input)\nencoder_lstm = LSTM(rnn_units)(encoder_embedding)\n\ndecoder_input = Input(shape=(max_sequence_length,))\ndecoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_input)\ndecoder_lstm = LSTM(rnn_units, return_sequences=True)(decoder_embedding)\ndecoder_output = Dense(vocab_size, activation='softmax')(decoder_lstm)\n\n# Crear el modelo codificador-decodificador\nmodel = Model([encoder_input, decoder_input], decoder_output)\n\n# Compilar el modelo\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Ajustar la forma de train_steps_padded\ntrain_steps_padded = np.pad(train_steps_padded, [(0, 0), (0, 1)], mode='constant', constant_values=0)\n\n# Entrenar el modelo con más épocas (por ejemplo, 50 épocas)\n#model.fit([train_ingredients_padded, train_steps_padded[:, :-1]], train_steps_padded[:, 1:], epochs=50, batch_size=32)\n\nmodel.fit([train_ingredients_padded, train_steps_padded[:, :-1]], train_steps_padded[:, 1:], epochs=15, batch_size=32)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T20:19:33.532629Z","iopub.execute_input":"2023-09-17T20:19:33.533589Z","iopub.status.idle":"2023-09-17T21:08:04.563576Z","shell.execute_reply.started":"2023-09-17T20:19:33.533547Z","shell.execute_reply":"2023-09-17T21:08:04.562443Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/15\n313/313 [==============================] - 196s 598ms/step - loss: 2.0023 - accuracy: 0.9188\nEpoch 2/15\n313/313 [==============================] - 188s 602ms/step - loss: 0.9013 - accuracy: 0.9218\nEpoch 3/15\n313/313 [==============================] - 190s 606ms/step - loss: 0.8782 - accuracy: 0.9219\nEpoch 4/15\n313/313 [==============================] - 189s 605ms/step - loss: 0.8608 - accuracy: 0.9220\nEpoch 5/15\n313/313 [==============================] - 190s 607ms/step - loss: 0.8391 - accuracy: 0.9221\nEpoch 6/15\n313/313 [==============================] - 190s 606ms/step - loss: 0.8153 - accuracy: 0.9222\nEpoch 7/15\n313/313 [==============================] - 190s 607ms/step - loss: 0.7868 - accuracy: 0.9223\nEpoch 8/15\n313/313 [==============================] - 190s 608ms/step - loss: 0.7550 - accuracy: 0.9224\nEpoch 9/15\n313/313 [==============================] - 190s 608ms/step - loss: 0.7219 - accuracy: 0.9226\nEpoch 10/15\n313/313 [==============================] - 190s 608ms/step - loss: 0.6885 - accuracy: 0.9228\nEpoch 11/15\n313/313 [==============================] - 190s 608ms/step - loss: 0.6553 - accuracy: 0.9229\nEpoch 12/15\n313/313 [==============================] - 190s 608ms/step - loss: 0.6227 - accuracy: 0.9232\nEpoch 13/15\n313/313 [==============================] - 190s 606ms/step - loss: 0.5914 - accuracy: 0.9235\nEpoch 14/15\n313/313 [==============================] - 190s 606ms/step - loss: 0.5612 - accuracy: 0.9240\nEpoch 15/15\n313/313 [==============================] - 190s 607ms/step - loss: 0.5315 - accuracy: 0.9250\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7aeb7b750a90>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Guardar y descargar el modelo y el tokenizer**","metadata":{}},{"cell_type":"code","source":"\nmodel.save('/kaggle/working/modelo_token_words.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:08:12.971630Z","iopub.execute_input":"2023-09-17T21:08:12.971988Z","iopub.status.idle":"2023-09-17T21:08:13.669570Z","shell.execute_reply.started":"2023-09-17T21:08:12.971959Z","shell.execute_reply":"2023-09-17T21:08:13.668455Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:08:17.622368Z","iopub.execute_input":"2023-09-17T21:08:17.622724Z","iopub.status.idle":"2023-09-17T21:08:17.629268Z","shell.execute_reply.started":"2023-09-17T21:08:17.622696Z","shell.execute_reply":"2023-09-17T21:08:17.628318Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"download_file('/kaggle/working/modelo_token_words.h5', 'model')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:08:21.519470Z","iopub.execute_input":"2023-09-17T21:08:21.519832Z","iopub.status.idle":"2023-09-17T21:08:38.742394Z","shell.execute_reply.started":"2023-09-17T21:08:21.519802Z","shell.execute_reply":"2023-09-17T21:08:38.741140Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\n# Guardar el tokenizer\nwith open('/kaggle/working/tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:11:37.815467Z","iopub.execute_input":"2023-09-17T21:11:37.815863Z","iopub.status.idle":"2023-09-17T21:11:37.947087Z","shell.execute_reply.started":"2023-09-17T21:11:37.815830Z","shell.execute_reply":"2023-09-17T21:11:37.945802Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:17:29.351503Z","iopub.execute_input":"2023-09-17T21:17:29.351940Z","iopub.status.idle":"2023-09-17T21:17:29.377516Z","shell.execute_reply.started":"2023-09-17T21:17:29.351907Z","shell.execute_reply":"2023-09-17T21:17:29.376671Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 200)]        0           []                               \n                                                                                                  \n embedding_1 (Embedding)        (None, 200, 100)     12320300    ['input_2[0][0]']                \n                                                                                                  \n lstm_1 (LSTM)                  (None, 200, 128)     117248      ['embedding_1[0][0]']            \n                                                                                                  \n input_1 (InputLayer)           [(None, 200)]        0           []                               \n                                                                                                  \n dense (Dense)                  (None, 200, 123203)  15893187    ['lstm_1[0][0]']                 \n                                                                                                  \n==================================================================================================\nTotal params: 28,330,735\nTrainable params: 28,330,735\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# para poder imprimir en párrafos\n!pip install textwrap3","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:17:34.061381Z","iopub.execute_input":"2023-09-17T21:17:34.061768Z","iopub.status.idle":"2023-09-17T21:17:48.960949Z","shell.execute_reply.started":"2023-09-17T21:17:34.061737Z","shell.execute_reply":"2023-09-17T21:17:48.959679Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: textwrap3 in /opt/conda/lib/python3.10/site-packages (0.9.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Generación de Recetas con ingredientes aleatorios**","metadata":{}},{"cell_type":"code","source":"import textwrap\ndef generate_recipe_from_random_ingredients(model, tokenizer, df, num_ingredients=5, max_steps=10, end_token='<end>', temperature=0.8):\n    # Obtener la lista completa de tokens de ingredientes\n    all_ingredients = df['ingredient_tokens'].tolist()\n    all_tokens = [token for ingredients in all_ingredients for token in ingredients]\n\n    # Seleccionar 5 tokens aleatorios\n    random_tokens = random.sample(all_tokens, num_ingredients)\n\n    print(\"Ingredientes seleccionados:\")\n    for i, token in enumerate(random_tokens):\n        english_ingredient = token\n        print(f\"Ingrediente {i+1}: {english_ingredient}\")\n\n\n\n\n    start_token = '<start>'\n    step_sequence = [tokenizer.word_index[start_token]]\n\n    # Convertir los tokens seleccionados a secuencias numéricas utilizando el tokenizer\n    random_tokens_sequence = tokenizer.texts_to_sequences([random_tokens])[0]\n\n    # Encontrar la receta más larga entre las seleccionadas y usarla como longitud máxima\n    max_recipe_length = min(max_steps, len(random_tokens_sequence) + 1)\n    max_recipe_length = 200\n\n    # Rellenar o truncar la secuencia de ingredientes para que tenga la misma longitud que max_recipe_length\n    initial_ingredients_list = pad_sequences([random_tokens_sequence], maxlen=max_recipe_length , padding='post', truncating='post')\n    \n\n    # Generar los pasos de la receta\n    for _ in range(max_steps - 1):\n        step_sequence_input = pad_sequences([step_sequence], maxlen=max_recipe_length, padding='post', truncating='post')\n        #next_token_probs = model.predict([initial_ingredients_list, step_sequence_input])[0, len(step_sequence_input[0]) - 1]\n        next_token_probs = model.predict([initial_ingredients_list, step_sequence_input])[0, -1]\n\n        # Utilizar temperatura para suavizar la distribución de probabilidad\n        next_token_probs = softmax(next_token_probs)\n        next_token_index = np.random.choice(len(next_token_probs), p=next_token_probs)\n\n        # Verificar que el índice generado esté dentro del rango válido del vocabulario\n        while next_token_index == 0 or tokenizer.index_word.get(next_token_index) is None:\n            next_token_index = np.random.choice(len(next_token_probs), p=next_token_probs)\n\n        next_token = tokenizer.index_word[next_token_index]\n        step_sequence.append(next_token_index)\n\n        if next_token == end_token:\n            break\n\n    # Convertir los índices de las palabras a texto, excluyendo el token de inicio\n    recipe_text = ' '.join(tokenizer.index_word.get(idx, '') for idx in step_sequence[1:])\n\n    \n    return recipe_text\n\n# Utilizando la función para generar una receta con 5 ingredientes aleatorios\nrecipe = generate_recipe_from_random_ingredients(model, tokenizer, df, num_ingredients=5, max_steps=8)\n# Imprimir la receta con wordwrap\nprint(\"\\nReceta generada:\")\nwrapper = textwrap.TextWrapper(width=80)\nlines = wrapper.wrap(recipe)\nfor line in lines:\n    print(line)\nprint('\\n')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:26:08.211652Z","iopub.execute_input":"2023-09-17T21:26:08.212090Z","iopub.status.idle":"2023-09-17T21:26:09.667481Z","shell.execute_reply.started":"2023-09-17T21:26:08.212055Z","shell.execute_reply":"2023-09-17T21:26:09.666367Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Ingredientes seleccionados:\nIngrediente 1: olive oil\nIngrediente 2: water\nIngrediente 3: crystallized ginger\nIngrediente 4: sugar\nIngrediente 5: raisins\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n\nReceta generada:\nslice brisket across grain and pour remaining barbecue sauce over meat raisin\ngather up the edges of the brie in place of flour use 2 cups rye cover and\nsimmer on low heat for 20 minutes egg bread and bacon over pitas and sprinkle\nwith cheese\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Utilizando la función para generar una receta con 5 ingredientes aleatorios\nrecipe = generate_recipe_from_random_ingredients(model, tokenizer, df, num_ingredients=5, max_steps=8)\n# Imprimir la receta con wordwrap\nprint(\"\\nReceta generada:\")\nwrapper = textwrap.TextWrapper(width=80)\nlines = wrapper.wrap(recipe)\nfor line in lines:\n    print(line)\nprint('\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:27:21.058856Z","iopub.execute_input":"2023-09-17T21:27:21.059235Z","iopub.status.idle":"2023-09-17T21:27:22.458782Z","shell.execute_reply.started":"2023-09-17T21:27:21.059202Z","shell.execute_reply":"2023-09-17T21:27:22.457781Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Ingredientes seleccionados:\nIngrediente 1: red pepper\nIngrediente 2: tart apples\nIngrediente 3: butter\nIngrediente 4: coconut milk\nIngrediente 5: raisins\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n\nReceta generada:\nfood grinder or processor sprinkle the dried bread crumbs over the top heat oil\nin a large pan and quickly pour the cornmeal batter into the skillet pour out\nthe drippings and wipe clean chop the walnuts or until cake test done\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Utilizando la función para generar una receta con 5 ingredientes aleatorios\nrecipe = generate_recipe_from_random_ingredients(model, tokenizer, df, num_ingredients=3, max_steps=8)\n# Imprimir la receta con wordwrap\nprint(\"\\nReceta generada:\")\nwrapper = textwrap.TextWrapper(width=80)\nlines = wrapper.wrap(recipe)\nfor line in lines:\n    print(line)\nprint('\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:28:09.326111Z","iopub.execute_input":"2023-09-17T21:28:09.326516Z","iopub.status.idle":"2023-09-17T21:28:10.793764Z","shell.execute_reply.started":"2023-09-17T21:28:09.326485Z","shell.execute_reply":"2023-09-17T21:28:10.792679Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Ingredientes seleccionados:\nIngrediente 1: orange juice\nIngrediente 2: butter\nIngrediente 3: egg\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n\nReceta generada:\nbake at 400 degrees for 7-8 minutes fold in 1 / 2 cup of the apples place an\napple slice between each biscuit and around the outer edge of the baking dish\nremove bones at 375 for 20 to 25 minutes dry oregano shake well and strain into\na pitcher or a chilled martini glass\n\n\n","output_type":"stream"}]}]}